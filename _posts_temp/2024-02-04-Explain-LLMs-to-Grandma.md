---
layout: post
title: Chat, G, P, T
# subtitle: AI大模型, 预训练, 微调, 蒸馏etc.
# cover-img: /assets/img/passion.jpg
tags: [LLMs, AI]
# comments: true
---

### 人工智能模型在推理阶段都做些什么？

人工智能模型的工作分为训练(training)和推理(inference)两个阶段，在跟人类聊天时，模型处于推理阶段，此时其不再调整自己的参数，而是根据已经学习到的知识来进行预测和响应，以帮助人类完成各种各样的任务。

具体来说， 在跟人类聊天时， 人工智能系统会执行以下步骤的工
作：
1.接收输入： 接收人类的输入， 通常是一句话或一段文字。 多模态
大模型还可以接收图片作为输入。
2.处理输入： 将输入的文本编码成数字向量， 以便计算机理解和处
理。
在把输入的内容传送到大模型做推理之前， 系统会先对输入进行检
测和预筛， 针对不合规、 不合法或不符合道德的有害问题， 直接拒绝回
答； 针对特定的、 不该随意发挥的问题， 直接给出官方标准回答。
3.进行推理： 模型会基于输入的文本使用已经训练好的神经网络模
型和它在之前的对话中所学到的知识来进行推理， 找到最有可能的响
应。

■ChatGPT会将人类输入的文本作为上文， 预测下一个标识(token)
或下一个单词序列。 具体来说， ChatGPT会将上文编码成一个数字向
量， 并将该向量输入到模型的解码器中。 解码器会根据该向量生成一个
初始的“开始” 符号， 并一步步生成下一个token或下一个单词序列，
直到遇到一个“结束” 符号或达到最大长度限制为止。
■ChatGPT使用了基于自回归(auto-regressive)的生成模型， 也就
是说， 在生成每个token时， 它都会考虑前面已经生成的token。 这种
方法可以保证生成文本的连贯性和语义一致性。 同时， ChatGPT也使用
了束搜索(beam search)等技术来计算多个概率较高的token候选集，
生成多个候选响应， 并选择其中概率最高的响应作为最终的输出。

4.生成输出： 将推理结果转换为自然语言， 以便人类理解， 这通常
是一句话或一段文字。
模型生成的回答文本也可能会经过系统中的合规性检测模块， 确保
输出内容符合要求， 再输出给人类。
·中文和英文的最小预测单元有什么不同？
上文故事里我们以中文为例， 模型会预测下一个字， 实际上模型是
预测下一个token， 在中文的条件下， 一个token等于一个汉字。

在处理英文时， 模型通常会使用分词技术将句子中的单词分割出
来， 并将每个单词作为一个token进行处理。 此外， 模型也可以使用更
细粒度的子词级别的token表示方法， 以便更好地利用单词内部的信
息。 这种方法在处理一些英文中常见的缩写、 不规则形式和新词时可能
会更加有效， 还能压缩词库中词的数量。

预测下一个字的概率时， 一定会选最高概率的字吗？

在生成token时， 模型通常会将解码器输出的每个token的概率归
一化， 并根据概率选择一个token作为生成的下一个单词或标点符号。
如果只选择概率最高的token， 生成的响应会比较保守和重复。 因此，
ChatGPT通常会使用温度(temperature)参数来引入一定程度的随机
性， 以使生成的响应更加丰富多样。 就像掷骰子一样， 在概率高于临界
点的token里面随机选择， 概率较高的token被选中的可能性较大。
应用程序编程接口的开发者可以根据实际场景的特点和需求， 对
temperature值进行调整。 通常情况下， 较大的temperature值会有更
多机会选择非最高概率token， 可以产生更多样的响应， 但也可能会导
致生成的响应过于随机和不合理。 相反， 较小的temperature值可以产
生更保守和合理的响应， 但也可能会导致生成的响应缺乏多样性。

### ChatGPT

Chat， 会聊天的； G——generative， 生成式； P——pretrained， 预训练； T——Transformer， 一种新型的神经网络。 

#### Pretrained 预训练

预训练是如何让模型越来越好的？
在上文的故事中， 我们将预训练比喻为神经网络模型的练习题。 在
预训练期间， 模型会根据预测结果进行反向传播， 调整模型参数以提高
模型的准确性。 这个过程与做练习题类似， 每一次训练都是为了让模型
更好地掌握语言知识和技能， 提高下一次预测的准确性。
当我们训练神经网络模型时， 通常需要对模型的参数进行优化， 以
使模型在预测任务中表现得更加准确。 这个过程被称为“反向传播” ，
核心思想是利用误差信号来更新模型参数， 以让模型能够更好地拟合训
练数据。 误差信号是指预测输出与实际输出之间的差异， 也就是我们希
望模型能够减小的损失函数。

在反向传播过程中， 我们首先将一个输入样本输入到神经网络模型
中， 然后计算出模型的预测输出。 接着， 我们将通过一个损失函数对预
测输出与实际输出之间的误差进行量化， 由此得到一个误差值。 这个误
差值会通过一个反向传播算法， 逐层向后传播到模型的每一个参数， 以
便计算每个参数对误差的贡献度。 我们可以使用链式法则将误差信号沿
着神经网络的层次结构传递回去， 以便计算每个参数的梯度。 在得到每
个参数的梯度之后， 我们可以利用梯度下降算法来更新模型的参数。 梯
度下降算法会根据参数梯度的方向和大小， 对模型参数进行微调， 从而
减小误差信号并提高模型的预测准确性。 这个过程与一个小球在山坡上
滚动类似， 根据山坡的斜率和重力的作用， 小球会沿着最陡峭的方向滚
动， 直到达到山底。

通过反向传播和梯度下降算法的迭代， 我们可以不断地调整模型参
数， 提高模型在训练数据上的表现， 并为模型的预测任务提供更准确的
结果。 这个过程是深度学习中非常重要的一部分， 也是神经网络模型能
够学习和优化的关键。

#### Generative

GPT与BERT的不同体现在哪里？
1.GPT是单向编码， BERT是双向编码。 GPT是基于Transformer解
码器构建的， 而BERT是基于Transformer编码器构建的。 这意味着GPT
只能利用左侧的上文信息， 而BERT可以同时利用左右两侧的上下文信
息， 可以捕捉更长距离的依赖关系， 并且更适合处理一词多义的情况。
2.GPT使用传统的语言模型作为预训练任务， 即根据前面的词预测
下一个词。 而BERT使用了两个预训练任务： 掩码语言模型(MLM)， 即
在输入中随机遮盖一些词， 然后根据上下文来还原这些词； 下一句预测
(NSP)， 即给定两个句子， 判断它们是否有连贯的关系。 这两个任务可
以提高BERT对语言结构和语义的理解能力。
3.GPT可以应用于自然语言理解(NLU)和自然语言生成(NLG)两大
任务， 而原生的BERT只能完成NLU任务， 无法直接应用在文本生成上
面。 这是因为GPT采用了左到右的解码器， 可以在未完整输入时预测接
下来的词汇。 而BERT没有解码器， 只能对输入进行编码和预测掩码位
置的词汇。


#### Chat


