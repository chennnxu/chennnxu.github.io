---
layout: post
title: Chat, G, P, T
# subtitle: AI大模型, 预训练, 微调, 蒸馏etc.
# cover-img: /assets/img/passion.jpg
tags: [LLMs, AI]
# comments: true
---

### 人工智能模型在推理阶段都做些什么？

人工智能模型的工作分为训练(training)和推理(inference)两个阶段，在跟人类聊天时，模型处于推理阶段，此时其不再调整自己的参数，而是根据已经学习到的知识来进行预测和响应，以帮助人类完成各种各样的任务。

具体来说， 在跟人类聊天时， 人工智能系统会执行以下步骤的工
作：
1.接收输入： 接收人类的输入， 通常是一句话或一段文字。 多模态
大模型还可以接收图片作为输入。
2.处理输入： 将输入的文本编码成数字向量， 以便计算机理解和处
理。
在把输入的内容传送到大模型做推理之前， 系统会先对输入进行检
测和预筛， 针对不合规、 不合法或不符合道德的有害问题， 直接拒绝回
答； 针对特定的、 不该随意发挥的问题， 直接给出官方标准回答。
3.进行推理： 模型会基于输入的文本使用已经训练好的神经网络模
型和它在之前的对话中所学到的知识来进行推理， 找到最有可能的响
应。

■ChatGPT会将人类输入的文本作为上文， 预测下一个标识(token)
或下一个单词序列。 具体来说， ChatGPT会将上文编码成一个数字向
量， 并将该向量输入到模型的解码器中。 解码器会根据该向量生成一个
初始的“开始” 符号， 并一步步生成下一个token或下一个单词序列，
直到遇到一个“结束” 符号或达到最大长度限制为止。
■ChatGPT使用了基于自回归(auto-regressive)的生成模型， 也就
是说， 在生成每个token时， 它都会考虑前面已经生成的token。 这种
方法可以保证生成文本的连贯性和语义一致性。 同时， ChatGPT也使用
了束搜索(beam search)等技术来计算多个概率较高的token候选集，
生成多个候选响应， 并选择其中概率最高的响应作为最终的输出。

4.生成输出： 将推理结果转换为自然语言， 以便人类理解， 这通常
是一句话或一段文字。
模型生成的回答文本也可能会经过系统中的合规性检测模块， 确保
输出内容符合要求， 再输出给人类。
·中文和英文的最小预测单元有什么不同？
上文故事里我们以中文为例， 模型会预测下一个字， 实际上模型是
预测下一个token， 在中文的条件下， 一个token等于一个汉字。

在处理英文时， 模型通常会使用分词技术将句子中的单词分割出
来， 并将每个单词作为一个token进行处理。 此外， 模型也可以使用更
细粒度的子词级别的token表示方法， 以便更好地利用单词内部的信
息。 这种方法在处理一些英文中常见的缩写、 不规则形式和新词时可能
会更加有效， 还能压缩词库中词的数量。

预测下一个字的概率时， 一定会选最高概率的字吗？

在生成token时， 模型通常会将解码器输出的每个token的概率归
一化， 并根据概率选择一个token作为生成的下一个单词或标点符号。
如果只选择概率最高的token， 生成的响应会比较保守和重复。 因此，
ChatGPT通常会使用温度(temperature)参数来引入一定程度的随机
性， 以使生成的响应更加丰富多样。 就像掷骰子一样， 在概率高于临界
点的token里面随机选择， 概率较高的token被选中的可能性较大。
应用程序编程接口的开发者可以根据实际场景的特点和需求， 对
temperature值进行调整。 通常情况下， 较大的temperature值会有更
多机会选择非最高概率token， 可以产生更多样的响应， 但也可能会导
致生成的响应过于随机和不合理。 相反， 较小的temperature值可以产
生更保守和合理的响应， 但也可能会导致生成的响应缺乏多样性。

### ChatGPT

Chat， 会聊天的； G——generative， 生成式； P——pretrained， 预训练； T——Transformer， 一种新型的神经网络。 

#### Pretrained 预训练

题海战术
预训练是如何让模型越来越好的？
在上文的故事中， 我们将预训练比喻为神经网络模型的练习题。 在
预训练期间， 模型会根据预测结果进行反向传播， 调整模型参数以提高
模型的准确性。 这个过程与做练习题类似， 每一次训练都是为了让模型
更好地掌握语言知识和技能， 提高下一次预测的准确性。
当我们训练神经网络模型时， 通常需要对模型的参数进行优化， 以
使模型在预测任务中表现得更加准确。 这个过程被称为“反向传播” ，
核心思想是利用误差信号来更新模型参数， 以让模型能够更好地拟合训
练数据。 误差信号是指预测输出与实际输出之间的差异， 也就是我们希
望模型能够减小的损失函数。

在反向传播过程中， 我们首先将一个输入样本输入到神经网络模型
中， 然后计算出模型的预测输出。 接着， 我们将通过一个损失函数对预
测输出与实际输出之间的误差进行量化， 由此得到一个误差值。 这个误
差值会通过一个反向传播算法， 逐层向后传播到模型的每一个参数， 以
便计算每个参数对误差的贡献度。 我们可以使用链式法则将误差信号沿
着神经网络的层次结构传递回去， 以便计算每个参数的梯度。 在得到每
个参数的梯度之后， 我们可以利用梯度下降算法来更新模型的参数。 梯
度下降算法会根据参数梯度的方向和大小， 对模型参数进行微调， 从而
减小误差信号并提高模型的预测准确性。 这个过程与一个小球在山坡上
滚动类似， 根据山坡的斜率和重力的作用， 小球会沿着最陡峭的方向滚
动， 直到达到山底。

通过反向传播和梯度下降算法的迭代， 我们可以不断地调整模型参
数， 提高模型在训练数据上的表现， 并为模型的预测任务提供更准确的
结果。 这个过程是深度学习中非常重要的一部分， 也是神经网络模型能
够学习和优化的关键。


ChatGPT在监督学习阶段使用的技术叫SFT(Supervised FineTuning)， 这跟传统的监督学习有什么不同？

监督学习是一种经典的机器学习方法， 其目标是使用有标签数据集
来训练一个模型， 以使其能够对新的未标记数据进行预测。 在监督学习
中， 训练数据的标签是已知的， 并且模型的目标是最小化预测输出与真
实标签之间的差异， 以学习如何进行准确的预测。
SFT监督微调是一种特定的迁移学习方法， 与传统从零开始训练的
监督学习有一些不同之处。 它基于一个通用的预训练模型， 使用少量有
标签的数据集对模型进行微调， 以适应特定任务的要求， 而不是像监督
学习一样从头开始训练一个模型。 微调方法通常需要更少的标签数据来
实现良好的性能， 因为预先训练的模型已经学习了一些通用的语言表
示， 可以更好地适应新的任务。 微调需要的训练时间和算力也更少， 在
微调过程中， 预训练模型的一部分可能会被固定， 以避免过度调整和过
拟合， 只会改变模型的一小部分层。
微调(Fine-Tuning)的起源可以追溯到早期的计算机视觉领域， 当
时在大型图像数据集上训练的卷积神经网络(CNN)被证明能够捕捉图像
中的高级特征， 这些特征在许多视觉任务中都是有用的。
不过， SFT仍然属于一种监督学习， 在微调过程中， 仍然需要有标
签的数据集来监督。 而且， 覆盖全面、 分布合理、 标注质量高的数据集
仍然是此类大模型的重要启动成本之一。 在ChatGPT出现之后， 理论
上， 其他模型可以抓取ChatGPT的高质量问答数据， 作为新模型微调数
据集的一部分。


#### Generative

GPT与BERT的不同体现在哪里？
1.GPT是单向编码， BERT是双向编码。 GPT是基于Transformer解
码器构建的， 而BERT是基于Transformer编码器构建的。 这意味着GPT
只能利用左侧的上文信息， 而BERT可以同时利用左右两侧的上下文信
息， 可以捕捉更长距离的依赖关系， 并且更适合处理一词多义的情况。
2.GPT使用传统的语言模型作为预训练任务， 即根据前面的词预测
下一个词。 而BERT使用了两个预训练任务： 掩码语言模型(MLM)， 即
在输入中随机遮盖一些词， 然后根据上下文来还原这些词； 下一句预测
(NSP)， 即给定两个句子， 判断它们是否有连贯的关系。 这两个任务可
以提高BERT对语言结构和语义的理解能力。
3.GPT可以应用于自然语言理解(NLU)和自然语言生成(NLG)两大
任务， 而原生的BERT只能完成NLU任务， 无法直接应用在文本生成上
面。 这是因为GPT采用了左到右的解码器， 可以在未完整输入时预测接
下来的词汇。 而BERT没有解码器， 只能对输入进行编码和预测掩码位
置的词汇。


#### Chat


别忘了还有我呢！ 老莫， 我来给你总结一下。
你读过海量的书和网页， 做过海量的文字接龙习题， 这叫生成式预训
练， 这阶段你积累了大量的知识和技能， 相当于有了排山倒海的浑厚内
力， 但还不会用、 用不好； 然后呢， 你跟师父学了薄薄的一本《问答宝
典》 ， 这叫监督学习， 这阶段你开始逐渐解锁一些技能， 知道人类会出
哪些招， 自己要应什么招； 再后来， 师父训练了一个奖励模型来陪你，
又做了大量的人类反馈强化学习， 这之后， 你的回答就越来越像人话
了！

我听说， 人类教
育孩子也是类似的过程。 孩子小时候， 默默地听大人说话， 自己看书，
先积累了大量的语料。 上学了， 老师上课讲例题， 让孩子做配套练习，是监督学习。 课后， 各种作业和考试， 有人给打分， 做对了奖励、 加
强， 做错了吃一堑长一智、 改正， 这是强化学习。 


·ChatGPT的RLHF跟当年AlphaGo的强化学习(RL)有什么不同？
RLHF是一种通过人类反馈来指导模型学习的方法， 而AlphaGo的
RL是一种基于强化学习的自主学习方法， 两者在奖励函数、 数据来源和
算法等方面都有明显的区别。
1.奖励函数和数据来源： 在RLHF中， 人类反馈被视为奖励信号，
数据来源于人类反馈， 需要人类专家的参与； 而在AlphaGo的RL中，
奖励函数是由自我对弈的棋局结果定义的， 数据是通过自己跟自己对弈
产生的， 因此不需要人类专家的参与， 成本更低， 可以无限量增加。
2.算法的使用： RLHF与AlphaGo使用的强化学习算法都源自于经
典的策略梯度(policy-gradient)分支， RLHF使用的是OpenAI自研的改
进策略优化(PPO)算法， 通过对比新旧策略来计算策略更新的方向和大
小， 并使用剪切范围来限制策略更新的大小， 以确保在学习过程中不会
引起太大的震荡。 与之相比， AlphaGo的算法则在优化过程中引入了蒙
特卡洛树搜索(MCTS)策略， 通过不断探索可能的行动序列来找到最优
策略， 使用树结构来组织搜索过程， 并通过统计模拟来评估行动的价
值。

RLHF的标记数据的数量和质量很重要， 但是， 奖励模型的训练数据只
能从专职的人类数据标记员那里输入吗？

专业的数据标记员为奖励模型在起步阶段提供输入。 当ChatGPT得
到广泛使用之后， 便可以利用真实用户的数据作为新的训练输入， 例如
用户对某条回答不满意， 点击“regenerate” 来重新生成一次回答
后， ChatGPT会询问用户， 新老两个回答哪个更好？ 这就获得了一个回
答质量排序数据
此外， 在垂直行业领域， 也需要更多的行业内专业人员进行标注，
如律师、 医生等， 才能使ChatGPT在专业领域获得更好的训练输入。



#### ICL(In-Context Learning)
大模型基于用户在提示文本中给出的少
量示例来进行预测。用户用
聊天的语言， 给出很少几个示例， 我就能用类比的方法来学会， 在我的
回答里举一反三， 举三反N。是有点像， 但监督学习是我师父提前准备好一批数据， 让
我批量学， 至少要成百上千的量才值得学， 而ICL是任何一个人类在跟
我聊天时都可以临时举例让我学。 另外， 跟以前的预训练、 监督学习、
强化学习都不同， ICL并不会修改我的模型参数， 也不需要消耗多少时
间和算力。 刚才小毕说学到老、 改造到老， 其实ICL没有改造我， 我只
是在当前的上下文中快速学习， 学以致用， 用完即止。


神秘的ICL究竟是怎么发生的， 模型从Context里面学到了什么？

ICL是指大模型能够从输入的文本中理解和捕获语言结构、 语义信
息和上下文关系。
坦率地说， 业界现在并不知道其原理， 甚至还有争议， 怀疑ICL到
底算不算一种学习。
在一些针对ICL表现的黑盒研究中， 有一些有趣的发现：
1.提示语中， 如果给出了错误的标签示例， 对学习的效果影响不
大， 这说明ICL并不像传统的监督学习那样， 去学习输入和标签（x和
y） 之间的对应关系。
2.提示语中给出的输入或标签的分布， 对学习的效果有明显影响，
这说明ICL学到了示例的语料分布和语言表达风格。
3.基于上述发现， 对ICL能干什么、 不能干什么， 我们可以有一个
重新的认识。

#### 思维链(CoT)
比如， 人类用户还可以在提示语中教我拆解任务， 教我一
步步往下走， 我都能马上学习他的逻辑， 在回答中体现出来。


### 大模型的未解之谜

#### 涌现

谷歌、斯坦福和DeepMind公司联合发表的《大语言模型的涌现能力》
(Emergent Abilities of Large Language Models)论文说， 许多新的
能力在中小模型上线性放大规模都得不到线性的增长， 模型规模必须要
指数级增长超过某个临界点， 新技能才会突飞猛进。
这么神奇！ 是不是有点像河面结冰， 让河面能走人的规
律？ 从夏天到冬天， 气温从35℃降到1℃， 水分子运动逐渐变慢， 但水
仍然是水， 河面仍然不能走人； 再从1℃降到0℃， 水分子一下子形成冰
晶， 河面突然能走人了！ 这个0℃冰点就像是大模型学新技能的规模临
界点？
没错， 这就是他们论文里说的涌现——由定量变化产生的
定性变化。 如果某种能力在较小的模型中不存在， 只在较大的模型中存
在， 这种能力就属于涌现能力。

凯文·凯利
1994年所著的《失控： 机器、 社会与经济的新生物学》 就讲到了这种
情况， 即关于复杂系统的进化、 涌现和失控。 ”

#### 幻觉

我搜到了一篇OpenAI的《GPT-4技术报告》 ， 里面重点
提到了大模型的幻觉(Hallucination)。 其中说GPT-4有产生幻觉的倾
向， 即‘产生与某些来源无关的荒谬或不真实的内容’ 

莫： “是啊， 我在RLHF阶段接受RM模型训练的时候， 为了拿到
RM的高分， 就想尽办法生成大多数人喜欢的回答咯。 ”
小毕： “所以AI业界对老莫跟人类对齐程度(Alignment)的评价特
别高。 也有人为幻觉辩护， 例如微软必应聊天机器人团队中的米哈伊尔
·帕拉欣(Mikhail Parakhin)就认为幻觉等于创造力， 大模型试图利用它
掌握的所有数据， 产生最连贯的句子， 不管是对是错， 但如果你压制住
这种幻觉和创造力， 大模型只能回答我不知道， 或者只能像搜索一样给
你现存的一模一样的信息。

《GPT-4技术报告》 里说， 随着模
型变得越来越有说服力和可信度， 幻觉倾向就会变得特别有害。 而且，
当模型变得更真实时， 幻觉可能变得更危险。 因为当模型在用户熟悉的
领域提供真实信息时， 用户会建立对模型的信任。 随着这些模型被融入
社会并用于帮助自动化各种系统， 幻觉倾向就会导致整体信息质量下
降， 降低公开可用信息的真实性和可信度。 还有， OpenAI公司的首席
科学家伊利亚也多次表示， 要想将大模型应用到更重要的场景中去来产
生价值， 幻觉是其最大的问题。 ”

我帮老莫解释一下什么是开放域和封闭域的幻觉。 封闭域
幻觉是指人类用户要求大模型仅使用给定背景中提供的信息， 但大模型
却创造了背景中没有的额外信息。 例如， 如果人类要求大模型对一篇文
章进行总结， 而大模型的回答里包含了文章中没有的信息， 这就属于封
闭域幻觉。 相比之下， 开放域幻觉是指大模型在没有参考任何特定输入
背景的情况下， 提供了关于世界的错误信息。 ”

牛津大学和OpenAI的联合论文《TruthfulQA： 评估模型如何模仿人类的错误》 中引用了一句话——‘The enemy of truth is blind
acceptance.’ （盲目接受是真理的敌人） 

#### 多模态

 Multimodal！ 老莫已经荣升为多模态大模
型了， 多模态大模型的英文全称是Multimodal Large Language
Model， 英文简称是MLLM。

